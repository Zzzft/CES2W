{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b3e568a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/fz/finetune/lora_env/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Sliding Window Attention is enabled but not implemented for `sdpa`; unexpected results may be encountered.\n",
      "Loading checkpoint shards: 100%|██████████| 4/4 [00:06<00:00,  1.59s/it]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Qwen2ForCausalLM(\n",
       "  (model): Qwen2Model(\n",
       "    (embed_tokens): Embedding(152064, 3584)\n",
       "    (layers): ModuleList(\n",
       "      (0-27): 28 x Qwen2DecoderLayer(\n",
       "        (self_attn): Qwen2Attention(\n",
       "          (q_proj): Linear(in_features=3584, out_features=3584, bias=True)\n",
       "          (k_proj): Linear(in_features=3584, out_features=512, bias=True)\n",
       "          (v_proj): Linear(in_features=3584, out_features=512, bias=True)\n",
       "          (o_proj): Linear(in_features=3584, out_features=3584, bias=False)\n",
       "        )\n",
       "        (mlp): Qwen2MLP(\n",
       "          (gate_proj): Linear(in_features=3584, out_features=18944, bias=False)\n",
       "          (up_proj): Linear(in_features=3584, out_features=18944, bias=False)\n",
       "          (down_proj): Linear(in_features=18944, out_features=3584, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): Qwen2RMSNorm((3584,), eps=1e-06)\n",
       "        (post_attention_layernorm): Qwen2RMSNorm((3584,), eps=1e-06)\n",
       "      )\n",
       "    )\n",
       "    (norm): Qwen2RMSNorm((3584,), eps=1e-06)\n",
       "    (rotary_emb): Qwen2RotaryEmbedding()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=3584, out_features=152064, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 加载原始模型\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "\n",
    "mode_path = './model/Qwen/Qwen2___5-7B-Instruct'  # 原始模型路径\n",
    "\n",
    "# 加载tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(mode_path, trust_remote_code=True)\n",
    "tokenizer.padding_side = \"left\"\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# 加载原始的CausalLM模型（不加载LoRA权重）\n",
    "model = AutoModelForCausalLM.from_pretrained(mode_path, device_map=\"auto\", torch_dtype=torch.bfloat16, trust_remote_code=True).eval()\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39f5b5fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "训练集: 8003条 (80.0%)\n",
      "测试集: 2001条 (20.0%)\n"
     ]
    }
   ],
   "source": [
    "# 加载训练集和测试集\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from datasets import Dataset\n",
    "\n",
    "df = pd.read_json('./dataset/merged_data.json')\n",
    "\n",
    "train_df, test_df = train_test_split(df, test_size=0.2, random_state=42)\n",
    "\n",
    "train_ds = Dataset.from_pandas(train_df)\n",
    "#val_ds = Dataset.from_pandas(val_df)\n",
    "test_ds = Dataset.from_pandas(test_df)\n",
    "\n",
    "# 5. 验证划分比例\n",
    "print(f\"训练集: {len(train_ds)}条 ({len(train_ds)/len(df):.1%})\")\n",
    "#print(f\"验证集: {len(val_ds)}条 ({len(val_ds)/len(df):.1%})\")\n",
    "print(f\"测试集: {len(test_ds)}条 ({len(test_ds)/len(df):.1%})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "81268cc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2001\n",
      "{'id': 2819, 'file_id': 163, 'spoken_text': '我没有退休费，老头有退休费，我们俩都是年龄大要的孩子，孩子才40多岁你想想。', 'context': '他已经83岁了，我82岁，我们都已年过八旬。我没有退休费，老头有退休费，我们俩都是年龄大要的孩子，孩子才40多岁你想想。这位82岁的奶奶，为什么没有退休金呢，40岁了才有孩子，听听奶奶是怎么说的。蒲扇挺好的，蒲扇这多少年了这，这我记得我小时候我奶奶就扇着这个哄我睡觉。哈哈哈哈，今天我们还发呢，我们这80岁以上的这给我们一人一把。您80岁了，可不你看这一脸这这老人斑，看您这个身体挺好挺健康的哈。', 'written_text': '我没有退休金，丈夫有退休金，我们俩都是年纪较大时才要的孩子，孩子现在才40多岁。', 'error_type': [1, 2, 4], '__index_level_0__': 2818}\n"
     ]
    }
   ],
   "source": [
    "print(len(test_ds))  # 检查测试集样本数量\n",
    "print(test_ds[0])   # 查看第一条数据内容"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc93552f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['id', 'file_id', 'spoken_text', 'context', 'written_text', 'error_type', '__index_level_0__'],\n",
       "    num_rows: 2001\n",
       "})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 测试集\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "from datasets import Dataset\n",
    "\n",
    "df = pd.read_json('./dataset/test_dataset.json')\n",
    "\n",
    "test_ds = Dataset.from_pandas(df)\n",
    "test_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75c361d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing samples: 100%|██████████| 251/251 [07:21<00:00,  1.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 预测结果已保存到 /home/fz/finetune/model_predictions/test_predictions_qwen.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# 保存模型预测结果\n",
    "import torch\n",
    "import random\n",
    "import json\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "\n",
    "import re\n",
    "\n",
    "def extract_translation_and_error_type(pred_text):\n",
    "    # 提取“翻译结果”，支持在“错误类型”前或单独一行\n",
    "    trans_match = re.search(r\"翻译结果[:：]?\\s*(.*?)(?:\\n|错误类型[:：])\", pred_text, re.DOTALL)\n",
    "    translation = trans_match.group(1).strip() if trans_match else \"\"\n",
    "\n",
    "    # 提取“错误类型”文本块（支持多种位置）\n",
    "    error_type_block_match = re.search(r\"错误类型[:：]([^\\n]*)\", pred_text)\n",
    "    error_type_line = error_type_block_match.group(1).strip() if error_type_block_match else \"\"\n",
    "\n",
    "    # 提取数字（支持中文逗号、英文逗号、空格、句号等混合格式）\n",
    "    # 如：1,3,4 或 1. 句子成分缺失：... 或 1，2，3\n",
    "    nums = re.findall(r\"[1-4]\", error_type_line)\n",
    "    error_type = sorted(set(int(n) for n in nums))\n",
    "\n",
    "    return translation, error_type\n",
    "\n",
    "\n",
    "# ✅ 设置 tokenizer 左侧 padding（适用于 decoder-only 架构）\n",
    "tokenizer.padding_side = 'left'\n",
    "\n",
    "test_samples =list(test_ds)\n",
    "\n",
    "batch_size = 8\n",
    "all_results = []\n",
    "\n",
    "for i in tqdm(range(0, len(test_samples), batch_size), desc=\"Processing samples\"):\n",
    "    batch = test_samples[i:i+batch_size]\n",
    "\n",
    "    # ✅ 构造输入\n",
    "    prompts = [\n",
    "        f\"\"\"<|im_start|>system\n",
    "            你是一位老年服务机构的文书编辑，擅长将老人的口头叙述准确、清晰地转化为日常书面风格的文本，并判断语句中存在的错误类型。\n",
    "            句子中可能存在的错误类型：1. 句子成分缺失。2. 句子结构混乱。3. 句子成分错误。4. 句子成分冗余。<|im_end|>\n",
    "            <|im_start|>user\n",
    "            原文：{example['spoken_text']}\n",
    "            上下文(仅协助理解，不翻译): {example['context']}\n",
    "            仅输出原文那一句话的翻译结果和错误类型序号，不要输出思考过程，不要输出解释。请在你预测的翻译结果前写“翻译结果：”，错误类型前写“错误类型：”。输出格式：\n",
    "            翻译结果：\n",
    "            错误类型：<|im_end|>\n",
    "            <|im_start|>assistant\"\"\" for example in batch\n",
    "    ]\n",
    "\n",
    "    inputs = tokenizer(\n",
    "        prompts,\n",
    "        return_tensors=\"pt\",\n",
    "        padding=\"longest\",\n",
    "        truncation=True,\n",
    "    ).to(model.device)\n",
    "\n",
    "    # ✅ 推理\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=256,\n",
    "            num_beams=1,\n",
    "            do_sample=False,\n",
    "            temperature=0.2,\n",
    "            top_p=0.95,\n",
    "        )\n",
    "\n",
    "    outputs = outputs[:, inputs['input_ids'].shape[1]:]\n",
    "    preds = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
    "    \n",
    "\n",
    "    # ✅ 处理每条预测结果\n",
    "    for pred_text, example in zip(preds, batch):\n",
    "        \n",
    "        #pred_text= pred_text.replace(\"gMASK\", \"\").strip()  # 去除无效标记\n",
    "        #print(pred_text)\n",
    "        translation, predicted_error_type = extract_translation_and_error_type(pred_text)\n",
    "\n",
    "        # 转换原始标签为数组\n",
    "        true_error = example.get(\"error_type\", [])\n",
    "        if isinstance(true_error, int):\n",
    "            true_error = [true_error]\n",
    "        elif isinstance(true_error, str):\n",
    "            true_error = [int(x) for x in re.findall(r\"\\d+\", true_error)]\n",
    "\n",
    "        result = {\n",
    "            \"spoken_text\": example[\"spoken_text\"],\n",
    "            \"context\": example[\"context\"],\n",
    "            \"reference\": example[\"written_text\"],\n",
    "            \"ref_error_type\": true_error,\n",
    "            \"prediction\": translation,\n",
    "            \"pred_error_type\": predicted_error_type\n",
    "        }\n",
    "        all_results.append(result)\n",
    "\n",
    "# ✅ 保存结果到 JSON 文件\n",
    "output_file = \"./lora/base_qwen.json\"\n",
    "with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(all_results, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "print(f\"✅ 预测结果已保存到 {output_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d12629b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/2001 [00:00<?, ?it/s]Building prefix dict from the default dictionary ...\n",
      "DEBUG:jieba:Building prefix dict from the default dictionary ...\n",
      "Loading model from cache /tmp/jieba.cache\n",
      "DEBUG:jieba:Loading model from cache /tmp/jieba.cache\n",
      "Loading model cost 0.425 seconds.\n",
      "DEBUG:jieba:Loading model cost 0.425 seconds.\n",
      "Prefix dict has been built successfully.\n",
      "DEBUG:jieba:Prefix dict has been built successfully.\n",
      "  0%|          | 1/2001 [00:00<15:07,  2.20it/s]Token indices sequence length is longer than the specified maximum sequence length for this model (150 > 128). Running this sequence through the model will result in indexing errors\n",
      "100%|██████████| 2001/2001 [00:18<00:00, 108.97it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "各项指标的平均值：\n",
      "BLEU-1: 0.6423\n",
      "BLEU-2: 0.3974\n",
      "BLEU-3: 0.2573\n",
      "BLEU-4: 0.1727\n",
      "ROUGE-1: 0.5718\n",
      "ROUGE-2: 0.2647\n",
      "ROUGE-L: 0.5094\n",
      "BLEURT: 0.3244\n",
      "Joint Accuracy: 0.1299\n",
      "Acc-1: 0.8056\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# 评估模型预测结果\n",
    "import json\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "\n",
    "# 加载模型和 tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"./bleurt-base-128\", local_files_only=True)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"./bleurt-base-128\", local_files_only=True)\n",
    "model.eval()\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "# 加载数据\n",
    "with open(\"./lora/base_qwen.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "import importlib\n",
    "import evaluate\n",
    "importlib.reload(evaluate)  # 强制重新加载评估函数  \n",
    "from evaluate import calculate_all_metrics\n",
    "\n",
    "# 存储所有指标\n",
    "all_scores = []\n",
    "\n",
    "for item in tqdm(data):\n",
    "    reference = item[\"reference\"]\n",
    "    generated = item[\"prediction\"]\n",
    "    if not reference or not generated:\n",
    "        continue  # 跳过空文本样本\n",
    "    ref_error_types = item.get(\"ref_error_type\", [])\n",
    "    pred_error_types = item.get(\"pred_error_type\", [])\n",
    "\n",
    "    metrics = calculate_all_metrics(reference, generated, tokenizer, model, ref_error_types, pred_error_types, device)\n",
    "    all_scores.append(metrics)\n",
    "\n",
    "# 计算每个指标的平均值\n",
    "average_scores = {}\n",
    "for key in all_scores[0].keys():\n",
    "    average_scores[key] = np.mean([score[key] for score in all_scores])\n",
    "\n",
    "# 打印平均结果\n",
    "print(\"各项指标的平均值：\")\n",
    "for key, value in average_scores.items():\n",
    "    print(f\"{key}: {value:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lora_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
